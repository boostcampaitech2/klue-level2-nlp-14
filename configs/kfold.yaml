# Data Arguments
DataArguments:
  name: jinmang2/load_klue_re
  revision: v3.0.1
  data_cache_dir: cache
  collator_name: default
  augment: aug1

# TrainingArguments
TrainingArguments:
  loss: focal
  trainer_class: custom
  report_to: wandb
  run_name: 1007_klue_roberta_lstm_aug_3000_kfold
  output_dir: output
  learning_rate: 3e-5
  do_train: True
  do_eval: True
  do_predict: True
  evaluation_strategy: epoch
  save_strategy: epoch
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  num_train_epochs: 4
  # eval_steps: 100
  # save_steps: 100
  save_total_limit: 5
  fp16: True
  weight_decay: 0.0
  warmup_ratio: 0.0
  load_best_model_at_end: True
  metric_for_best_model: micro_f1
  logging_dir: logs
  lr_scheduler_type: linear

# Model Arguments
ModelingArguments:
  architectures: RobertaForSequenceClassificationLstm
  model_name_or_path: jinmang2/roberta-large-re-tapt-20300
  model_cache_dir: cache

# Project Arguments
ProjectArguments:
  task: klue_re
  wandb_project: klue_re_klue_roberta_taeuk
  save_model_dir: best_models/1007_klue_roberta_lstm_aug_3000_kfold
  submit_dir: prediction
  checkpoint: None