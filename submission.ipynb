{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "495ec825-8337-4b95-92cd-9c3418c95ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "998863ef-9a8c-4292-ae32-5569404ce614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "778dac1b-79e9-4ceb-9368-54dca0bf2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "  subject_entity = []\n",
    "  object_entity = []\n",
    "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    i = i[1:-1].split(',')[0].split(':')[1]\n",
    "    j = j[1:-1].split(',')[0].split(':')[1]\n",
    "\n",
    "    subject_entity.append(i)\n",
    "    object_entity.append(j)\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
    "  return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25ed4a1d-e0c4-47e3-a548-4fb7312ecef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "  \"\"\"\n",
    "    test dataset을 DataLoader로 만들어 준 후,\n",
    "    batch_size로 나눠 model이 예측 합니다.\n",
    "  \"\"\"\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  output_prob = []\n",
    "  for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "      outputs = model(\n",
    "          input_ids=data['input_ids'].to(device),\n",
    "          attention_mask=data['attention_mask'].to(device),\n",
    "          token_type_ids=data['token_type_ids'].to(device)\n",
    "          )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "  return np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2322c44b-5547-48c0-9df4-b7f7cd4712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(label):\n",
    "  \"\"\"\n",
    "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
    "  \"\"\"\n",
    "  origin_label = []\n",
    "  with open('dict_num_to_label.pkl', 'rb') as f:\n",
    "    dict_num_to_label = pickle.load(f)\n",
    "  for v in label:\n",
    "    origin_label.append(dict_num_to_label[v])\n",
    "  \n",
    "  return origin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68744775-752a-4f0d-b1cd-ba7309a9e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "  \"\"\"\n",
    "    test dataset을 불러온 후,\n",
    "    tokenizing 합니다.\n",
    "  \"\"\"\n",
    "  test_dataset = load_data(dataset_dir)\n",
    "  test_label = list(map(int,test_dataset['label'].values))\n",
    "  # tokenizing dataset\n",
    "  tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "  return test_dataset['id'], tokenized_test, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "489afd90-f683-4173-8a3a-2d15e9ed77fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:35<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Finish! ----\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "  \"\"\"\n",
    "    주어진 dataset csv 파일과 같은 형태일 경우 inference 가능한 코드입니다.\n",
    "  \"\"\"\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  # load tokenizer\n",
    "  Tokenizer_NAME = \"klue/bert-base\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
    "\n",
    "  ## load my model\n",
    "  # MODEL_NAME = args.model_di # model dir.\n",
    "  MODEL_NAME = \"./best_model\" # model dir.\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "  model.parameters\n",
    "  model.to(device)\n",
    "\n",
    "  ## load test datset\n",
    "  test_dataset_dir = \"../dataset/test/test_data.csv\"\n",
    "  test_id, test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "  Re_test_dataset = RE_Dataset(test_dataset ,test_label)\n",
    "\n",
    "  ## predict answer\n",
    "  pred_answer, output_prob = inference(model, Re_test_dataset, device) # model에서 class 추론\n",
    "  pred_answer = num_to_label(pred_answer) # 숫자로 된 class를 원래 문자열 라벨로 변환.\n",
    "  \n",
    "  ## make csv file with predicted answer\n",
    "  #########################################################\n",
    "  # 아래 directory와 columns의 형태는 지켜주시기 바랍니다.\n",
    "  output = pd.DataFrame({'id':test_id,'pred_label':pred_answer,'probs':output_prob,})\n",
    "\n",
    "  output.to_csv('./prediction/submission.csv', index=False) # 최종적으로 완성된 예측한 라벨 csv 파일 형태로 저장.\n",
    "  #### 필수!! ##############################################\n",
    "  print('---- Finish! ----')\n",
    "if __name__ == '__main__':\n",
    "#   parser = argparse.ArgumentParser()\n",
    "  \n",
    "#   # model dir\n",
    "#   parser.add_argument('--model_dir', type=str, default=\"./best_model\")\n",
    "#   args = parser.parse_args()\n",
    "#   print(args)\n",
    "  args=1\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abfb17-82e5-4f63-86e9-f169e5dacd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe02ae-b28a-4579-9f86-fe06cb78d5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
