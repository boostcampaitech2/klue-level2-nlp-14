{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fe3ddbb-52e0-4622-8316-26451170a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "30735662-570b-440e-b419-1d7d09729a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "578a55fb-b3f6-4670-9a80-1b7ed3599bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤.\"\"\"\n",
    "  subject_entity = []\n",
    "  object_entity = []\n",
    "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    i = i[1:-1].split(',')[0].split(':')[1]\n",
    "    j = j[1:-1].split(',')[0].split(':')[1]\n",
    "\n",
    "    subject_entity.append(i)\n",
    "    object_entity.append(j)\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label']})\n",
    "  return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64453aa8-deb8-4da5-bec5-d2fbaf55bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv íŒŒì¼ì„ ê²½ë¡œì— ë§¡ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf27c1b5-94cb-415a-962e-bbb0fed53320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2edce7b-49e1-491d-98ea-00c54dd1eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('/opt/ml/dataset/train/train.csv')\n",
    "label = label_to_num(dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc1f336f-158b-44f2-8ea4-dc9ac97b2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_eval_idx.pkl', 'rb') as f:\n",
    "    train_eval_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8893c079-aad1-4150-8f0b-b820541e7ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'eval'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval_idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c44ee78-6e11-4a61-9ecf-83c456c5f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.iloc[train_eval_idx['train']]\n",
    "dev_dataset = dataset.iloc[train_eval_idx['eval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "89497027-21b9-4e7c-baba-60462a917313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.array(label)[train_eval_idx['train']]\n",
    "dev_label = np.array(label)[train_eval_idx['eval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "275d7778-91c1-4635-ab5b-f945e23e486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44f1e643-0c51-47bf-96ef-879adea77819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>10706</td>\n",
       "      <td>ì¶”ìŠ¹ìš°(ç§‹æ‰¿ä½‘, 1979ë…„ 9ì›” 24ì¼ ~)ëŠ” ì „ KBO ë¦¬ê·¸ í•œí™” ì´ê¸€ìŠ¤ì˜ ì™¸ì•¼ìˆ˜...</td>\n",
       "      <td>'ì¶”ìŠ¹ìš°'</td>\n",
       "      <td>'ì™¸ì•¼ìˆ˜'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26891</th>\n",
       "      <td>26891</td>\n",
       "      <td>ë¶€ì•ˆêµ°ì— ë”°ë¥´ë©´ ë¶€ì•ˆêµ°ìì›ë´‰ì‚¬ì„¼í„°ì™€ ì „ë¼ë¶ë„ìì›ë´‰ì‚¬ì„¼í„°ê°€ ê³µë™ìœ¼ë¡œ ì£¼ê´€í•œ ì´ë²ˆ í–‰ì‚¬...</td>\n",
       "      <td>'ë¶€ì•ˆêµ°'</td>\n",
       "      <td>'ì „ë¼ë¶ë„'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29437</th>\n",
       "      <td>29437</td>\n",
       "      <td>ì¤‘ë³µì¸ë ¥ ê°ì¶•, ì„œìš¸ë©”íŠ¸ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œ ë„ì‹œì² ë„ê³µì‚¬ ì„ì› ì¸ê±´ë¹„ ì ˆê°ìœ¼ë¡œ 2027ë…„...</td>\n",
       "      <td>'ì„œìš¸ë©”íŠ¸ë¡œ'</td>\n",
       "      <td>'ì„œìš¸íŠ¹ë³„ì‹œ'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29780</th>\n",
       "      <td>29780</td>\n",
       "      <td>ê·¸ ê²°ê³¼ ë¯¼ì£¼ì •ì˜ë‹¹ ëŒ€í‘œì¸ ë…¸íƒœìš°ê°€ ëŒ€í†µë ¹ ì§ì„ ì œ ê°œí—Œì„ ìˆ˜ìš©í•˜ëŠ” 6Â·29ì„ ì–¸ì´ ë°œ...</td>\n",
       "      <td>'ë¯¼ì£¼ì •ì˜ë‹¹'</td>\n",
       "      <td>'ë…¸íƒœìš°'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27887</th>\n",
       "      <td>27887</td>\n",
       "      <td>í•œí¸ ì°½ë‹¹ì— ì•ì„œ ì˜› ìœ ì‹ ë‹¹ì˜ ëŒ€í‘œì˜€ë˜ ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬ëŠ” ê³¼ê±° ë¯¼ì£¼ë‹¹ì„ íƒˆë‹¹í•œ ì „ë ¥...</td>\n",
       "      <td>'ìœ ì‹ ë‹¹'</td>\n",
       "      <td>'ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "10706  10706  ì¶”ìŠ¹ìš°(ç§‹æ‰¿ä½‘, 1979ë…„ 9ì›” 24ì¼ ~)ëŠ” ì „ KBO ë¦¬ê·¸ í•œí™” ì´ê¸€ìŠ¤ì˜ ì™¸ì•¼ìˆ˜...   \n",
       "26891  26891  ë¶€ì•ˆêµ°ì— ë”°ë¥´ë©´ ë¶€ì•ˆêµ°ìì›ë´‰ì‚¬ì„¼í„°ì™€ ì „ë¼ë¶ë„ìì›ë´‰ì‚¬ì„¼í„°ê°€ ê³µë™ìœ¼ë¡œ ì£¼ê´€í•œ ì´ë²ˆ í–‰ì‚¬...   \n",
       "29437  29437  ì¤‘ë³µì¸ë ¥ ê°ì¶•, ì„œìš¸ë©”íŠ¸ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œ ë„ì‹œì² ë„ê³µì‚¬ ì„ì› ì¸ê±´ë¹„ ì ˆê°ìœ¼ë¡œ 2027ë…„...   \n",
       "29780  29780  ê·¸ ê²°ê³¼ ë¯¼ì£¼ì •ì˜ë‹¹ ëŒ€í‘œì¸ ë…¸íƒœìš°ê°€ ëŒ€í†µë ¹ ì§ì„ ì œ ê°œí—Œì„ ìˆ˜ìš©í•˜ëŠ” 6Â·29ì„ ì–¸ì´ ë°œ...   \n",
       "27887  27887  í•œí¸ ì°½ë‹¹ì— ì•ì„œ ì˜› ìœ ì‹ ë‹¹ì˜ ëŒ€í‘œì˜€ë˜ ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬ëŠ” ê³¼ê±° ë¯¼ì£¼ë‹¹ì„ íƒˆë‹¹í•œ ì „ë ¥...   \n",
       "\n",
       "      subject_entity object_entity                      label  \n",
       "10706          'ì¶”ìŠ¹ìš°'         'ì™¸ì•¼ìˆ˜'                no_relation  \n",
       "26891          'ë¶€ì•ˆêµ°'        'ì „ë¼ë¶ë„'                no_relation  \n",
       "29437        'ì„œìš¸ë©”íŠ¸ë¡œ'       'ì„œìš¸íŠ¹ë³„ì‹œ'              org:member_of  \n",
       "29780        'ë¯¼ì£¼ì •ì˜ë‹¹'         'ë…¸íƒœìš°'  org:top_members/employees  \n",
       "27887          'ìœ ì‹ ë‹¹'    'ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬'  org:top_members/employees  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb600241-afdf-4e2f-8ce0-920f716fc62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /opt/ml/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "aa = tokenized_dataset(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b22128c3-a354-4790-85a1-4f9da532b9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " \"'\",\n",
       " 'ì¶”',\n",
       " '##ìŠ¹',\n",
       " '##ìš°',\n",
       " \"'\",\n",
       " '[SEP]',\n",
       " \"'\",\n",
       " 'ì™¸ì•¼ìˆ˜',\n",
       " \"'\",\n",
       " '[SEP]',\n",
       " 'ì¶”',\n",
       " '##ìŠ¹',\n",
       " '##ìš°',\n",
       " '(',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " ',',\n",
       " '1979',\n",
       " '##ë…„',\n",
       " '9',\n",
       " '##ì›”',\n",
       " '24',\n",
       " '##ì¼',\n",
       " '~',\n",
       " ')',\n",
       " 'ëŠ”',\n",
       " 'ì „',\n",
       " 'KBO',\n",
       " 'ë¦¬ê·¸',\n",
       " 'í•œí™”',\n",
       " 'ì´ê¸€ìŠ¤',\n",
       " '##ì˜',\n",
       " 'ì™¸ì•¼ìˆ˜',\n",
       " '##ì´',\n",
       " '##ì',\n",
       " ',',\n",
       " 'í˜„ì¬',\n",
       " 'KBO',\n",
       " 'ë¦¬ê·¸',\n",
       " 'í•œí™”',\n",
       " 'ì´ê¸€ìŠ¤',\n",
       " '##ì˜',\n",
       " 'ì‘ì „',\n",
       " 'Â·',\n",
       " 'ì£¼',\n",
       " '##ë£¨',\n",
       " 'ë°',\n",
       " 'ì™¸ì•¼ìˆ˜',\n",
       " 'ìˆ˜ë¹„',\n",
       " 'ì½”ì¹˜',\n",
       " '##ì´ë‹¤',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aa.items()\n",
    "# aa[0]\n",
    "aa[0].tokens\n",
    "# aa[0].ids\n",
    "# aa[0].attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22452a2b-1c32-4b90-8f0c-b9c88019472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset êµ¬ì„±ì„ ìœ„í•œ class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "637ec096-2446-467e-9dc3-847a8a6a865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515212eb-fd50-4232-b3f7-f2a935a7d619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f198d37-859c-4f94-80e2-f973b7b1ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f0acc146-7096-4c7d-8a93-ff77db8f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2198562-6475-4183-ac2b-f9f648fe4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  \"\"\" validationì„ ìœ„í•œ metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1(preds, labels)\n",
    "  auprc = klue_re_auprc(probs, labels)\n",
    "  acc = accuracy_score(labels, preds) # ë¦¬ë”ë³´ë“œ í‰ê°€ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280c530-08b9-4db9-a0bd-a12e08bb9f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "740e308f-81a9-459b-94ce-326453a6ee69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-57c0ec490b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-57c0ec490b2b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-57c0ec490b2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# load model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# MODEL_NAME = \"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"klue/bert-base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "  seed_everything(args.seed)\n",
    "  # load model and tokenizer\n",
    "  # MODEL_NAME = \"bert-base-uncased\"\n",
    "  MODEL_NAME = \"klue/bert-base\"\n",
    " \n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "  # load dataset\n",
    "  # train_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "  # dev_dataset = load_data(\"../dataset/train/dev.csv\") # validationìš© ë°ì´í„°ëŠ” ë”°ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "  # train_label = label_to_num(train_dataset['label'].values)\n",
    "  # dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "  wandb_project = \"klue_re_bert_ce\"\n",
    "  report_to = \"wandb\"\n",
    "  # tokenizing dataset\n",
    "  tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "  tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "  # make dataset for pytorch.\n",
    "  RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "  RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  print(device)\n",
    "  # setting model hyperparameter\n",
    "  model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "  model_config.num_labels = 30\n",
    "\n",
    "  model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)\n",
    "  print(model.config)\n",
    "  \n",
    "  model.parameters\n",
    "  model.to(device)\n",
    "\n",
    "\n",
    "  os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "  call_wandb = True\n",
    "  try:\n",
    "      os.environ[\"WANDB_PROJECT\"]\n",
    "      \n",
    "  except KeyError:\n",
    "      call_wandb = False\n",
    "      \n",
    "  if call_wandb:\n",
    "      import wandb\n",
    "      wandb.login()\n",
    "  \n",
    "  # ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "  # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps=500,                 # model saving step.\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    learning_rate=3e-5,               # learning_rate\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = 500,            # evaluation step.\n",
    "    load_best_model_at_end = True \n",
    "  )\n",
    "    \n",
    "  trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=RE_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_train_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics         # define metrics function\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train()\n",
    "  save_pretrained('./best_model')\n",
    "    \n",
    "    \n",
    "def main():\n",
    "  train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a364e1e-9334-4950-b6da-0703fcfae273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
