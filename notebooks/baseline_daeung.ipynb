{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65800bdc-7c31-48d7-a209-f115d1577e08",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66340fa9-a8df-4209-83b8-237fc1fc77c6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79737e08-767e-4bc3-9801-a397172c16d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9621656-e4e5-4e60-a3b3-42854e721cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2dcf9eb-03c3-47fc-8937-655b33cf3d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset klue_re (cache/klue_re/re/1.0.1/72db5b4f9111ca3106d23ecec67a53f750c70b70b32c5512f925baac3b39a0de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7245195bc51c4d6b99ba0c0626a73d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"jinmang2/load_klue_re\", script_version=\"v1.0.1b\", cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d138c903-94bd-4bda-b70b-20463f8ac494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('submission-baseline-0929-roberta-large-warmup0.2_lr3e-5-test.csv')\n",
    "df_old = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "408c5040-16ba-491f-96ae-5f56e868a2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8520283322601416"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_new.pred_label == df_old.pred_label).sum() / len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf335d7-ffba-48ee-bd1d-58502d80dc19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/klue_re\n"
     ]
    }
   ],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "from functools import partial\n",
    "from typing import Tuple, List, Any, Dict\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# from metrics import *\n",
    "# from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a5f50d-67a0-4569-9855-e406f90fe679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab41185-c81c-4bb2-b366-b9d64ced7bc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f6ee36-4fa8-49ca-88ec-5575a415e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a634308f-24f8-49ef-9ada-c186e830dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(class_names)}\n",
    "label2id = {label: idx for idx, label in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a8943-baba-4e17-a309-a2606513d255",
   "metadata": {},
   "source": [
    "KLUE에서 제시된 hyperparameter setting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6f5c3-112f-4300-81c0-2ed29036c859",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12983a4-0613-4738-8f81-fbbca60d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space\n",
    "# tune_config = {\n",
    "#     \"per_device_train_batch_size\": tune.choice([8, 16, 32]),\n",
    "#     \"per_device_eval_batch_size\": 32,\n",
    "#     \"learning_rate\": tune.choice([1e-5, 2e-5, 3e-5, 5e-5]),\n",
    "#     \"warmup_ratio\": tune.choice([0., 0.1, 0.2, 0.6]),\n",
    "#     \"weihgt_decay\": tune.choice([0.0, 0.01]),\n",
    "#     \"num_train_epochs\": tune.choice([3, 4, 5, 10]),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a19a5c-e37a-4d83-953a-2557870d7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patience = 10000\n",
    "output_dir = \"klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata\"\n",
    "wandb_project = 'klue_re_klue-roberta-large-daeug'\n",
    "run_name = 'baseline-0928-roberta-large-warmup0.2_lr3e-5-newdata'\n",
    "report_to = \"wandb\"\n",
    "max_length = 128\n",
    "cache_dir = 'cache'\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41ba912-f118-4724-b55e-551f95216327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8fe348-162e-4de3-b13f-c9cb3acf47d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2077b343b0b241a7ac12b405052a9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63df26195a6c4692a6c7ef44b59f6e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset klue_re/re (download: 4.91 MiB, generated: 12.41 MiB, post-processed: Unknown size, total: 17.33 MiB) to cache/klue_re/re/1.0.0/5edf8095b936bfcc409e00bd241e5f6dad3b4c6e21e1df9f42be7e4c1eb3918c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de436c4c6d5f426fa24ce4db48ca1a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset klue_re downloaded and prepared to cache/klue_re/re/1.0.0/5edf8095b936bfcc409e00bd241e5f6dad3b4c6e21e1df9f42be7e4c1eb3918c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e267e7c7931a417a99498a378a617e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Version 1.0.1 - train test split\n",
    "klue_re = datasets.load_dataset(\"jinmang2/load_klue_re\",script_version=\"v1.0.1\", cache_dir='cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630604a-4100-49f7-9076-bcee3cac59ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4712b-c83c-4417-8a89-59b12ba30087",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39100eb0-d599-452a-bf4e-fe3fccc1ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../dataset/train/train.csv\")\n",
    "label = label_to_num(dataset['label'].values)\n",
    "\n",
    "dataset['id'] = dataset['id'].astype(str)\n",
    "dataset['subject_entity'] = dataset['subject_entity'].apply(lambda x : eval(x))\n",
    "dataset['object_entity'] = dataset['object_entity'].apply(lambda x : eval(x))\n",
    "dataset['label'] = dataset['label'].map(label2id)\n",
    "\n",
    "with open('train_eval_idx.pkl', 'rb') as f:\n",
    "    train_eval_idx = pickle.load(f)\n",
    "    \n",
    "train_dataset = dataset.iloc[train_eval_idx['train']]\n",
    "dev_dataset = dataset.iloc[train_eval_idx['eval']]\n",
    "\n",
    "train_label = np.array(label)[train_eval_idx['train']]\n",
    "dev_label = np.array(label)[train_eval_idx['eval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56a22bd-ca42-4929-98e5-cc4685726be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "class_labels = ClassLabel(num_classes=30, names=class_names)\n",
    "\n",
    "hf_train_dataset = Dataset.from_pandas(train_dataset)\n",
    "hf_dev_dataset = Dataset.from_pandas(dev_dataset)\n",
    "\n",
    "hf_train_dataset = hf_train_dataset.remove_columns('__index_level_0__')\n",
    "hf_dev_dataset = hf_dev_dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "klue_re = DatasetDict({'train' : hf_train_dataset, 'dev' : hf_dev_dataset})\n",
    "\n",
    "klue_re['train'].features['label'] = ClassLabel(num_classes=30, names=class_names)\n",
    "klue_re['dev'].features['label'] = ClassLabel(num_classes=30, names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591a21c2-588d-420a-a72a-68f18d3156df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'subject_entity': {'end_idx': Value(dtype='int64', id=None),\n",
       "  'start_idx': Value(dtype='int64', id=None),\n",
       "  'type': Value(dtype='string', id=None),\n",
       "  'word': Value(dtype='string', id=None)},\n",
       " 'object_entity': {'end_idx': Value(dtype='int64', id=None),\n",
       "  'start_idx': Value(dtype='int64', id=None),\n",
       "  'type': Value(dtype='string', id=None),\n",
       "  'word': Value(dtype='string', id=None)},\n",
       " 'label': ClassLabel(num_classes=30, names=['no_relation', 'org:top_members/employees', 'org:members', 'org:product', 'per:title', 'org:alternate_names', 'per:employee_of', 'org:place_of_headquarters', 'per:product', 'org:number_of_employees/members', 'per:children', 'per:place_of_residence', 'per:alternate_names', 'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings', 'per:spouse', 'org:founded', 'org:political/religious_affiliation', 'org:member_of', 'per:parents', 'org:dissolved', 'per:schools_attended', 'per:date_of_death', 'per:date_of_birth', 'per:place_of_birth', 'per:place_of_death', 'org:founded_by', 'per:religion'], names_file=None, id=None),\n",
       " 'source': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_re['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8d9ac3-e9ec-4707-9f63-7636d064082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10706',\n",
       " 'sentence': '추승우(秋承佑, 1979년 9월 24일 ~)는 전 KBO 리그 한화 이글스의 외야수이자, 현재 KBO 리그 한화 이글스의 작전·주루 및 외야수 수비 코치이다.',\n",
       " 'subject_entity': {'end_idx': 2,\n",
       "  'start_idx': 0,\n",
       "  'type': 'PER',\n",
       "  'word': '추승우'},\n",
       " 'object_entity': {'end_idx': 78,\n",
       "  'start_idx': 76,\n",
       "  'type': 'POH',\n",
       "  'word': '외야수'},\n",
       " 'label': 0,\n",
       " 'source': 'wikipedia'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47782f2-0413-44cf-b360-85f699bbf274",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae12aacc-32f8-4c5c-bc9e-b245e320177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizing dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cff7dc7-7b93-4b49-89e7-93639bdf3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_example\n",
    "\n",
    "markers = dict(\n",
    "    subject_start_marker=\"<subj>\",\n",
    "    subject_end_marker=\"</subj>\",\n",
    "    object_start_marker=\"<obj>\",\n",
    "    object_end_marker=\"</obj>\",\n",
    ")\n",
    "\n",
    "def mark_entity_spans(examples,\n",
    "                      subject_start_marker: str, subject_end_marker: str,\n",
    "                      object_start_marker: str, object_end_marker: str):\n",
    "\n",
    "    def _mark_entity_spans(\n",
    "        text: str, \n",
    "        subject_range=Tuple[int, int], \n",
    "        object_range=Tuple[int, int]\n",
    "    ) -> str:\n",
    "        \"\"\" Adds entity markers to the text to identify the subject/object entities.\n",
    "        Args:\n",
    "            text: Original sentence\n",
    "            subject_range: Pair of start and end indices of subject entity\n",
    "            object_range: Pair of start and end indices of object entity\n",
    "        Returns:\n",
    "            A string of text with subject/object entity markers\n",
    "        \"\"\"\n",
    "        if subject_range < object_range:\n",
    "            segments = [\n",
    "                text[: subject_range[0]],\n",
    "                subject_start_marker,\n",
    "                text[subject_range[0] : subject_range[1] + 1],\n",
    "                subject_end_marker,\n",
    "                text[subject_range[1] + 1 : object_range[0]],\n",
    "                object_start_marker,\n",
    "                text[object_range[0] : object_range[1] + 1],\n",
    "                object_end_marker,\n",
    "                text[object_range[1] + 1 :],\n",
    "            ]\n",
    "        elif subject_range > object_range:\n",
    "            segments = [\n",
    "                text[: object_range[0]],\n",
    "                object_start_marker,\n",
    "                text[object_range[0] : object_range[1] + 1],\n",
    "                object_end_marker,\n",
    "                text[object_range[1] + 1 : subject_range[0]],\n",
    "                subject_start_marker,\n",
    "                text[subject_range[0] : subject_range[1] + 1],\n",
    "                subject_end_marker,\n",
    "                text[subject_range[1] + 1 :],\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"Entity boundaries overlap.\")\n",
    "\n",
    "        marked_text = \"\".join(segments)\n",
    "\n",
    "        return marked_text\n",
    "    \n",
    "    subject_entity = examples[\"subject_entity\"]\n",
    "    object_entity = examples[\"object_entity\"]\n",
    "    \n",
    "    text = _mark_entity_spans(\n",
    "        examples[\"sentence\"],\n",
    "        (subject_entity[\"start_idx\"], subject_entity[\"end_idx\"]),\n",
    "        (object_entity[\"start_idx\"], object_entity[\"end_idx\"]),\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "mark_entity_spans = partial(mark_entity_spans, **markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9665f77c-c49c-4135-b640-8e078bd240d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d615f400184b8da107773ee593d4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25976 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25b49ee8da74e428db043634bbd8408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6494 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = klue_re.map(mark_entity_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa36709-9cd7-4a77-b0d9-5e0d89942598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(markers.values())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27376a5b-a0be-4a8d-9b4d-15e6e34a443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(\n",
    "    examples, \n",
    "    tokenizer,\n",
    "    subject_start_marker: str,\n",
    "    subject_end_marker: str,\n",
    "    object_start_marker: str,\n",
    "    object_end_marker: str\n",
    ") -> Dict[str, List[Any]]:\n",
    "    \n",
    "    def fix_tokenization_error(text: str) -> List[str]:\n",
    "        \"\"\"Fix the tokenization due to the `obj` and `subj` marker inserted\n",
    "        in the middle of a word.\n",
    "        Example:\n",
    "            >>> text = \"<obj>조지 해리슨</obj>이 쓰고 <subj>비틀즈</subj>가\"\n",
    "            >>> tokens = ['<obj>', '조지', '해리', '##슨', '</obj>', '이', '쓰', '##고', '<subj>', '비틀즈', '</subj>', '가']\n",
    "            >>> fix_tokenization_error(text)\n",
    "            ['<obj>', '조지', '해리', '##슨', '</obj>', '##이', '쓰', '##고', '<subj>', '비틀즈', '</subj>', '##가']\n",
    "            \n",
    "        Only support for BertTokenizerFast\n",
    "        If you use bbpe, change code!\n",
    "        \"\"\"\n",
    "        batch_encoding = tokenizer._tokenizer.encode(text)\n",
    "        tokens = batch_encoding.tokens\n",
    "        # subject\n",
    "        if text[text.find(subject_end_marker) + len(subject_end_marker)] != \" \":\n",
    "            space_idx = tokens.index(subject_end_marker) + 1\n",
    "            # tokenizer_type == \"bert-wp\"\n",
    "            if not tokens[space_idx].startswith(\"##\") and \"가\" <= tokens[space_idx][0] <= \"힣\":\n",
    "                tokens[space_idx] = \"##\" + tokens[space_idx]\n",
    "\n",
    "        # object\n",
    "        if text[text.find(object_end_marker) + len(object_end_marker)] != \" \":\n",
    "            space_idx = tokens.index(object_end_marker) + 1\n",
    "            # tokenizer_type == \"bert-wp\"\n",
    "            if not tokens[space_idx].startswith(\"##\") and \"가\" <= tokens[space_idx][0] <= \"힣\":\n",
    "                tokens[space_idx] = \"##\" + tokens[space_idx]\n",
    "        \n",
    "        return tokens    \n",
    "    \n",
    "    tokens = fix_tokenization_error(examples[\"text\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer.convert_tokens_to_ids(tokens),\n",
    "        \"tokenized\": tokens,\n",
    "    }\n",
    "\n",
    "convert_example_to_features = partial(\n",
    "    convert_example_to_features,\n",
    "    tokenizer=tokenizer,\n",
    "    **markers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "986697f1-762f-41f7-91b7-ffa726c7dc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58480fd5603841df9a1eeef45ddf71e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25976 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ad8f4e59334dae86e318908c31c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6494 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = examples.map(convert_example_to_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d85b9-583f-4ed8-b887-eb1e01fefa8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e312e757-c22d-4089-aff8-54445ba27fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b8a60f7-6113-4fc0-a380-3eb2d1e993ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"cache_dir\": \"cache\",\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_relation\",\n",
      "    \"1\": \"org:top_members/employees\",\n",
      "    \"2\": \"org:members\",\n",
      "    \"3\": \"org:product\",\n",
      "    \"4\": \"per:title\",\n",
      "    \"5\": \"org:alternate_names\",\n",
      "    \"6\": \"per:employee_of\",\n",
      "    \"7\": \"org:place_of_headquarters\",\n",
      "    \"8\": \"per:product\",\n",
      "    \"9\": \"org:number_of_employees/members\",\n",
      "    \"10\": \"per:children\",\n",
      "    \"11\": \"per:place_of_residence\",\n",
      "    \"12\": \"per:alternate_names\",\n",
      "    \"13\": \"per:other_family\",\n",
      "    \"14\": \"per:colleagues\",\n",
      "    \"15\": \"per:origin\",\n",
      "    \"16\": \"per:siblings\",\n",
      "    \"17\": \"per:spouse\",\n",
      "    \"18\": \"org:founded\",\n",
      "    \"19\": \"org:political/religious_affiliation\",\n",
      "    \"20\": \"org:member_of\",\n",
      "    \"21\": \"per:parents\",\n",
      "    \"22\": \"org:dissolved\",\n",
      "    \"23\": \"per:schools_attended\",\n",
      "    \"24\": \"per:date_of_death\",\n",
      "    \"25\": \"per:date_of_birth\",\n",
      "    \"26\": \"per:place_of_birth\",\n",
      "    \"27\": \"per:place_of_death\",\n",
      "    \"28\": \"org:founded_by\",\n",
      "    \"29\": \"per:religion\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"no_relation\": 0,\n",
      "    \"org:alternate_names\": 5,\n",
      "    \"org:dissolved\": 22,\n",
      "    \"org:founded\": 18,\n",
      "    \"org:founded_by\": 28,\n",
      "    \"org:member_of\": 20,\n",
      "    \"org:members\": 2,\n",
      "    \"org:number_of_employees/members\": 9,\n",
      "    \"org:place_of_headquarters\": 7,\n",
      "    \"org:political/religious_affiliation\": 19,\n",
      "    \"org:product\": 3,\n",
      "    \"org:top_members/employees\": 1,\n",
      "    \"per:alternate_names\": 12,\n",
      "    \"per:children\": 10,\n",
      "    \"per:colleagues\": 14,\n",
      "    \"per:date_of_birth\": 25,\n",
      "    \"per:date_of_death\": 24,\n",
      "    \"per:employee_of\": 6,\n",
      "    \"per:origin\": 15,\n",
      "    \"per:other_family\": 13,\n",
      "    \"per:parents\": 21,\n",
      "    \"per:place_of_birth\": 26,\n",
      "    \"per:place_of_death\": 27,\n",
      "    \"per:place_of_residence\": 11,\n",
      "    \"per:product\": 8,\n",
      "    \"per:religion\": 29,\n",
      "    \"per:schools_attended\": 23,\n",
      "    \"per:siblings\": 16,\n",
      "    \"per:spouse\": 17,\n",
      "    \"per:title\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting model hyperparameter\n",
    "model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "model_config.cache_dir = 'cache'\n",
    "model_config.id2label = id2label\n",
    "model_config.label2id = label2id\n",
    "# model_config.max_length = 128\n",
    "\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n",
    "                                                            config=model_config,\n",
    "                                                            cache_dir='cache')\n",
    "print(model.config)\n",
    "model.parameters\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d893765b-0c29-4223-9052-d2407aaae10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resize...\n"
     ]
    }
   ],
   "source": [
    "# Vocab Resize\n",
    "if model.config.vocab_size < len(tokenizer):\n",
    "    print(\"resize...\")\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda1606-504f-436d-8242-4c10bf8c91b8",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbeedc7-cb29-488a-85d1-06a631af6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length='single'):\n",
    "        self.tokenizer = tokenizer\n",
    "        if max_length == 'single':\n",
    "            self.max_length = tokenizer.max_len_single_sentence\n",
    "        elif max_length == 'pair':\n",
    "            self.max_length = tokenizer.max_len_sentences_pair\n",
    "        elif type(max_length) == int:\n",
    "            self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        input_ids = [x[\"input_ids\"] for x in batch]\n",
    "        labels = [x[\"label\"] for x in batch]\n",
    "        batch_encoding = tokenizer.pad(\n",
    "            {\"input_ids\": input_ids},\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch_encoding.update({\"labels\": torch.LongTensor(labels)})\n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddf188c2-a17f-49b4-a8f6-fb10c3624b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollator(tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a20b353a-1acb-4ebb-8f0c-94e395617024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiyoung2\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "\n",
    "call_wandb = True\n",
    "try:\n",
    "    os.environ[\"WANDB_PROJECT\"]\n",
    "    \n",
    "except KeyError:\n",
    "    call_wandb = False\n",
    "    \n",
    "if call_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a93cdb8-37a2-4b3b-bba8-2df160c7af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "            \n",
    "            # Checkpoint\n",
    "            output_dir=output_dir,          # output directory\n",
    "#             save_total_limit=5,              # number of total save model.\n",
    "            save_strategy=\"epoch\",\n",
    "#             save_steps=500,                 # model saving step.\n",
    "            \n",
    "            # Training\n",
    "            num_train_epochs=4,              # total number of training epochs\n",
    "            learning_rate=3e-5,               # learning_rate\n",
    "            per_device_train_batch_size=32,  # batch size per device during training\n",
    "            per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "            \n",
    "            ## Learning rate scheduling\n",
    "#             lr_scheduler_type = 'cosine',\n",
    "#             warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "            warmup_ratio = 0.2,\n",
    "\n",
    "\n",
    "            ## Regularization\n",
    "            weight_decay=0.01,               # strength of weight decay\n",
    "\n",
    "            # Logging\n",
    "            logging_dir='./logs',            # directory for storing logs\n",
    "#             logging_steps=100,              # log saving step.\n",
    "            report_to = 'wandb',\n",
    "            \n",
    "            # Evaluation\n",
    "            metric_for_best_model = 'auprc',\n",
    "            evaluation_strategy='epoch', # evaluation strategy to adopt during training\n",
    "                                        # `no`: No evaluation during training.\n",
    "                                        # `steps`: Evaluate every `eval_steps`.\n",
    "                                        # `epoch`: Evaluate every end of epoch.\n",
    "            eval_steps = 500,            # evaluation step.\n",
    "            \n",
    "            # ETC    \n",
    "            load_best_model_at_end = True,\n",
    "            seed = 42,\n",
    "            \n",
    "            # Hugging Face Hub upload\n",
    "#             push_to_hub = True,\n",
    "#             push_to_hub_model_id = f\"{model_name}-finetuned-{task}\",\n",
    "    \n",
    "            # AMP\n",
    "            fp16 = True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1cddd5d-75e4-4883-a813-040382391797",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = list(tokenized_datasets[\"train\"].features.keys())\n",
    "features_name.pop(features_name.index(\"input_ids\"))\n",
    "features_name.pop(features_name.index(\"label\"))\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5341a453-8d4a-4db3-b404-0ee9eea3139d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids'],\n",
       "        num_rows: 25976\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['label', 'input_ids'],\n",
       "        num_rows: 6494\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cb5dd20-0e0f-4ffc-8758-9b60c9c0c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "            model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=tokenized_datasets['train'],         # training dataset\n",
    "            eval_dataset=tokenized_datasets['dev'],             # evaluation dataset\n",
    "#             eval_dataset=tokenized_datasets['dev'],             # evaluation dataset\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics         # define metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482f899-5581-4c8c-8442-dad472027603",
   "metadata": {},
   "source": [
    "# Loss & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd425ed-4597-4aa7-98c1-391b2e6076e9",
   "metadata": {},
   "source": [
    "- `metrics.py` 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828d826-5d3a-4ae1-b5c1-cb1a322c78ba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7de185d9-007d-4009-beab-c73ed295236a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dccd4ad-8ac9-443b-aaaf-31e2fc864535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23ozl9fw\n"
     ]
    }
   ],
   "source": [
    "id = wandb.util.generate_id()\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0b849-3b99-47df-8124-4ad7ee2d15ed",
   "metadata": {},
   "source": [
    "생성된 id를 붙여 넣으면 `wandb`를 사용할 수 있습니다.\n",
    "> **\\[참고\\]** - project : 실험기록을 관리할 프로젝트 이름. 없을 시 입력받은 이름으로 생성, 여기선 예시로 klue로 설정\n",
    "- entity : weights & biases 사용자명 또는 팀 이름\n",
    "- id : 실험에 부여된 고유 아이디\n",
    "- name : 실험에 부여한 이름\n",
    "- resume : 실험을 재개할 떄, 실험에 부여한 고유 아이디를 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05d7616b-5671-420c-85ab-9d3d43e49737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline-0928-roberta-large-warmup0.2_lr3e-5-newdata</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug\" target=\"_blank\">https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug/runs/23ozl9fw\" target=\"_blank\">https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug/runs/23ozl9fw</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/klue_re/wandb/run-20210928_013856-23ozl9fw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(23ozl9fw)</h1><iframe src=\"https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug/runs/23ozl9fw\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f718a620370>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=wandb_project, # 실험기록을 관리한 프로젝트 이름\n",
    "           entity='kiyoung2',     # 사용자명 또는 팀 이름\n",
    "           id='23ozl9fw',      # 실험에 부여된 고유 아이디\n",
    "           name=run_name,         # 실험에 부여한 이름\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ecaa58b-dc9d-4660-8f1b-790564b37ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 25976\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3248\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3248' max='3248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3248/3248 21:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.678500</td>\n",
       "      <td>0.669914</td>\n",
       "      <td>0.798275</td>\n",
       "      <td>0.717739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.844027</td>\n",
       "      <td>0.761848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.495992</td>\n",
       "      <td>0.859564</td>\n",
       "      <td>0.819069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.609471</td>\n",
       "      <td>0.860984</td>\n",
       "      <td>0.819533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/trainer.py:1347: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6494\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-812\n",
      "Configuration saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-812/config.json\n",
      "Model weights saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-812/pytorch_model.bin\n",
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6494\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-1624\n",
      "Configuration saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-1624/config.json\n",
      "Model weights saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-1624/pytorch_model.bin\n",
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n",
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/trainer.py:1347: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6494\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-2436\n",
      "Configuration saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-2436/config.json\n",
      "Model weights saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-2436/pytorch_model.bin\n",
      "/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6494\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-3248\n",
      "Configuration saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-3248/config.json\n",
      "Model weights saved in klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-3248/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from klue_dir/klue-roberta-large-warmup0.2_3e-5-newdata/checkpoint-3248 (score: 0.8195332634865903).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3248, training_loss=0.5932091069338944, metrics={'train_runtime': 1305.1997, 'train_samples_per_second': 79.608, 'train_steps_per_second': 2.489, 'total_flos': 2.4605941785440736e+16, 'train_loss': 0.5932091069338944, 'epoch': 4.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b934e61-7c0f-46ba-a17f-c535f7ba02f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27881<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/klue_re/wandb/run-20210927_165822-1zzv5cah/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/klue_re/wandb/run-20210927_165822-1zzv5cah/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/auprc</td><td>0.81436</td></tr><tr><td>eval/loss</td><td>0.61059</td></tr><tr><td>eval/micro_f1</td><td>0.85837</td></tr><tr><td>eval/runtime</td><td>50.4615</td></tr><tr><td>eval/samples_per_second</td><td>128.692</td></tr><tr><td>eval/steps_per_second</td><td>4.023</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>3248</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.182</td></tr><tr><td>train/total_flos</td><td>2.46133251276528e+16</td></tr><tr><td>train/train_loss</td><td>0.60853</td></tr><tr><td>train/train_runtime</td><td>1301.9444</td></tr><tr><td>train/train_samples_per_second</td><td>79.807</td></tr><tr><td>train/train_steps_per_second</td><td>2.495</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/auprc</td><td>▁▆██</td></tr><tr><td>eval/loss</td><td>█▃▁▄</td></tr><tr><td>eval/micro_f1</td><td>▁▆██</td></tr><tr><td>eval/runtime</td><td>█▁▆▇</td></tr><tr><td>eval/samples_per_second</td><td>▁█▃▂</td></tr><tr><td>eval/steps_per_second</td><td>▁█▄▂</td></tr><tr><td>train/epoch</td><td>▁▂▂▄▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▄▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>▇█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">baseline-0928-roberta-large-warmup0.2_lr3e-5</strong>: <a href=\"https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug/runs/1zzv5cah\" target=\"_blank\">https://wandb.ai/kiyoung2/klue_re_klue-roberta-large-daeug/runs/1zzv5cah</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93e0e6-51c7-4787-a22d-2e05b4cc4cdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5da67cdc-9b2d-4a00-bf5a-5bb3882d6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5d0ea9d-0f39-4fc9-bd14-de5fe055f5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20793c70-79da-4124-a95f-751935672536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory klue_dir/klue-roberta-large-warmup0.2_3e-5/ or `from_tf` and `from_flax` set to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11577/4113937816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# model_config.max_length = 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model =  AutoModelForSequenceClassification.from_pretrained(checkpoint,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                             cache_dir='cache')\n",
      "\u001b[0;32m/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         raise ValueError(\n\u001b[1;32m    421\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basic/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                     \u001b[0marchive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1267\u001b[0m                         \u001b[0;34mf\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                         \u001b[0;34mf\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory klue_dir/klue-roberta-large-warmup0.2_3e-5/ or `from_tf` and `from_flax` set to False."
     ]
    }
   ],
   "source": [
    "# setting model hyperparameter\n",
    "# checkpoint = './klue_dir/checkpoint-3248/'\n",
    "checkpoint = 'klue_dir/klue-roberta-large-warmup0.2_3e-5/'\n",
    "model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "model_config.cache_dir = 'cache'\n",
    "model_config.id2label = id2label\n",
    "model_config.label2id = label2id\n",
    "model_config.vocab_size = 32004\n",
    "# model_config.max_length = 128\n",
    "\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                            config=model_config,\n",
    "                                                            cache_dir='cache')\n",
    "print(model.config)\n",
    "model.parameters\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415717a7-3e23-4cc2-8cb3-d79bca210977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026da30e-627f-4da6-b98c-cc270569ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../dataset/test/test_data.csv\")\n",
    "# label = label_to_num(dataset['label'].values)\n",
    "\n",
    "dataset['id'] = dataset['id'].astype(str)\n",
    "dataset['subject_entity'] = dataset['subject_entity'].apply(lambda x : eval(x))\n",
    "dataset['object_entity'] = dataset['object_entity'].apply(lambda x : eval(x))\n",
    "# dataset['label'] = dataset['label'].map(label2id)\n",
    "\n",
    "# with open('train_eval_idx.pkl', 'rb') as f:\n",
    "#     train_eval_idx = pickle.load(f)\n",
    "    \n",
    "# train_dataset = dataset.iloc[train_eval_idx['train']]\n",
    "# dev_dataset = dataset.iloc[train_eval_idx['eval']]\n",
    "\n",
    "# train_label = np.array(label)[train_eval_idx['train']]\n",
    "# dev_label = np.array(label)[train_eval_idx['eval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d399920-1a0a-452e-b7bf-ba09b1a714c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ClassLabel(num_classes=30, names=class_names)\n",
    "\n",
    "hf_test_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# hf_test_dataset = hf_test_dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "hf_test_dataset.features['label'] = ClassLabel(num_classes=30, names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32016354-39b3-4995-b04a-2c93b54b7910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026df283b67b4854b4a93444746ea531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7765 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_examples = hf_test_dataset.map(mark_entity_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64f2962c-05f3-4d83-810b-76113caff128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3419d24def940b8aafcaf1262facb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7765 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_datasets = test_examples.map(convert_example_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9103aac8-1dd0-43ce-a872-d28efd2057eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(tokenized_test_datasets, collate_fn=data_collator, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "affb0320-0f32-4b82-a5bb-02dbd61b9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c85d0caa-7b2d-4f23-a52d-0bc32553766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  3625,  3749,  ...,     1,     1,     1],\n",
       "        [    0,  3784, 21154,  ...,     1,     1,     1],\n",
       "        [    0,  3995,    16,  ...,  2062,    18,     2],\n",
       "        ...,\n",
       "        [    0,  1504,  2259,  ...,     1,     1,     1],\n",
       "        [    0,  3753, 32000,  ...,     1,     1,     1],\n",
       "        [    0,  9500,  2079,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "        100, 100, 100, 100])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20e91ed7-5e3c-45b7-9dcc-87efc53514cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:56<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "for i, data in enumerate(tqdm(test_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "          input_ids=data['input_ids'].to(device),\n",
    "          attention_mask=data['attention_mask'].to(device),\n",
    "#           token_type_ids=data['token_type_ids'].to(device)\n",
    "          )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbc487ae-189b-4a6c-94df-523a86987cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ea54b8b-e299-4819-98c7-dcc403399820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(label):\n",
    "  \"\"\"\n",
    "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
    "  \"\"\"\n",
    "  origin_label = []\n",
    "  with open('dict_num_to_label.pkl', 'rb') as f:\n",
    "    dict_num_to_label = pickle.load(f)\n",
    "  for v in label:\n",
    "    origin_label.append(dict_num_to_label[v])\n",
    "  \n",
    "  return origin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9467153-134f-4b9d-8219-80a20aa4972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_answer = num_to_label(pred_answer) # 숫자로 된 class를 원래 문자열 라벨로 변환.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87129422-ef21-4172-80f5-06f2301b3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id':dataset['id'].values,'pred_label':pred_answer,'probs':output_prob,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3336af54-abf0-40e2-b67a-522313fdb0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>org:product</td>\n",
       "      <td>[0.015178636647760868, 0.0011734288418665528, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>per:alternate_names</td>\n",
       "      <td>[0.0028730828780680895, 0.00038902508094906807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.9991886019706726, 3.7351230275817215e-05, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.8227369785308838, 0.000364467705367133, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[0.13053059577941895, 0.8653919696807861, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>7760</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.4702913761138916, 0.00040200879448093474, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>7761</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.995478093624115, 0.000579211744479835, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>7762</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[0.00033924198942258954, 0.9989251494407654, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>7763</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.9862713813781738, 0.00011878733494086191, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>7764</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[0.9974666833877563, 0.0010241603013128042, 5....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7765 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                 pred_label  \\\n",
       "0        0                org:product   \n",
       "1        1        per:alternate_names   \n",
       "2        2                no_relation   \n",
       "3        3                no_relation   \n",
       "4        4  org:top_members/employees   \n",
       "...    ...                        ...   \n",
       "7760  7760                no_relation   \n",
       "7761  7761                no_relation   \n",
       "7762  7762  org:top_members/employees   \n",
       "7763  7763                no_relation   \n",
       "7764  7764                no_relation   \n",
       "\n",
       "                                                  probs  \n",
       "0     [0.015178636647760868, 0.0011734288418665528, ...  \n",
       "1     [0.0028730828780680895, 0.00038902508094906807...  \n",
       "2     [0.9991886019706726, 3.7351230275817215e-05, 3...  \n",
       "3     [0.8227369785308838, 0.000364467705367133, 0.0...  \n",
       "4     [0.13053059577941895, 0.8653919696807861, 0.00...  \n",
       "...                                                 ...  \n",
       "7760  [0.4702913761138916, 0.00040200879448093474, 0...  \n",
       "7761  [0.995478093624115, 0.000579211744479835, 0.00...  \n",
       "7762  [0.00033924198942258954, 0.9989251494407654, 2...  \n",
       "7763  [0.9862713813781738, 0.00011878733494086191, 5...  \n",
       "7764  [0.9974666833877563, 0.0010241603013128042, 5....  \n",
       "\n",
       "[7765 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9e4d2a5-9455-4b3d-8412-691575ee2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b77aee-ad56-43b0-81ce-0e824200010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "  \"\"\"\n",
    "    test dataset을 DataLoader로 만들어 준 후,\n",
    "    batch_size로 나눠 model이 예측 합니다.\n",
    "  \"\"\"\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  output_prob = []\n",
    "  for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "      outputs = model(\n",
    "          input_ids=data['input_ids'].to(device),\n",
    "          attention_mask=data['attention_mask'].to(device),\n",
    "          token_type_ids=data['token_type_ids'].to(device)\n",
    "          )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "  return np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6281eb-0b25-41fb-9ad5-284bd76f65a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da7d20-2865-4be3-9453-710e270d03e5",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2c082-7627-4862-a663-88f17631e3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
