{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe41a39-da4f-49ab-844b-33d663b5111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example is uses the official\n",
    "huggingface transformers `hyperparameter_search` API.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from transformers import AutoConfig, \\\n",
    "    AutoModelForSequenceClassification, AutoTokenizer, Trainer, GlueDataset, \\\n",
    "    GlueDataTrainingArguments, TrainingArguments\n",
    "\n",
    "from constant import *\n",
    "from entity_prep import *\n",
    "from metrics import * \n",
    "\n",
    "\n",
    "def tune_transformer(num_samples=8, gpus_per_trial=0, smoke_test=False):\n",
    "    data_dir_name = \"./hp_search\" if not smoke_test else \"./hp_search_test\"\n",
    "    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n",
    "    cache_dir = './cache'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir, 0o755)\n",
    "\n",
    "    # Change these as needed.\n",
    "    model_name = \"klue/roberta-large\" if not smoke_test \\\n",
    "        else \"klue/roberta-small\"\n",
    "    task_name = \"re\"\n",
    "\n",
    "    task_data_dir = os.path.join(data_dir, task_name.upper())\n",
    "\n",
    "    num_labels = len(CLASS_NAMES)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, num_labels=num_labels)\n",
    "\n",
    "    # setting model hyperparameter\n",
    "    model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 30\n",
    "    model_config.cache_dir = cache_dir\n",
    "    model_config.id2label = IDX2LABEL\n",
    "    model_config.label2id = LABEL2IDX\n",
    "\n",
    "    # Download and cache tokenizer, model, and features\n",
    "    print(\"Downloading and caching Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(markers.values())}\n",
    "    )\n",
    "    convert_example_to_features = partial(\n",
    "    convert_example_to_features,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Triggers tokenizer download to cache\n",
    "    print(\"Downloading and caching pre-trained model\")\n",
    "    AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    def get_model():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "\n",
    "    # Download data.\n",
    "    # klue_re = datasets.load_dataset(\"jinmang2/load_klue_re\",script_version=\"v1.0.1\", cache_dir='cache')\n",
    "#########\n",
    "    dataset = pd.read_csv(\"../dataset/train/train.csv\")\n",
    "    label = label_to_num(dataset['label'].values)\n",
    "\n",
    "    dataset['id'] = dataset['id'].astype(str)\n",
    "    dataset['subject_entity'] = dataset['subject_entity'].apply(lambda x : eval(x))\n",
    "    dataset['object_entity'] = dataset['object_entity'].apply(lambda x : eval(x))\n",
    "    dataset['label'] = dataset['label'].map(label2id)\n",
    "\n",
    "    with open('train_eval_idx.pkl', 'rb') as f:\n",
    "        train_eval_idx = pickle.load(f)\n",
    "\n",
    "    train_dataset = dataset.iloc[train_eval_idx['train']]\n",
    "    dev_dataset = dataset.iloc[train_eval_idx['eval']]\n",
    "\n",
    "    train_label = np.array(label)[train_eval_idx['train']]\n",
    "    dev_label = np.array(label)[train_eval_idx['eval']]\n",
    "    \n",
    "    from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "    class_labels = ClassLabel(num_classes=30, names=class_names)\n",
    "\n",
    "    hf_train_dataset = Dataset.from_pandas(train_dataset)\n",
    "    hf_dev_dataset = Dataset.from_pandas(dev_dataset)\n",
    "\n",
    "    hf_train_dataset = hf_train_dataset.remove_columns('__index_level_0__')\n",
    "    hf_dev_dataset = hf_dev_dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "    klue_re = DatasetDict({'train' : hf_train_dataset, 'dev' : hf_dev_dataset})\n",
    "\n",
    "    klue_re['train'].features['label'] = ClassLabel(num_classes=30, names=class_names)\n",
    "    klue_re['dev'].features['label'] = ClassLabel(num_classes=30, names=class_names)\n",
    "#########\n",
    "    \n",
    "    examples = klue_re.map(mark_entity_spans)\n",
    "    tokenized_datasets = examples.map(convert_example_to_features)\n",
    "\n",
    "# 사용한 option 외에도 다양한 option들이 있습니다.\n",
    "# https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments 참고해주세요.\n",
    "    training_args = TrainingArguments(\n",
    "                # Checkpoint\n",
    "                output_dir=\".\",\n",
    "                save_strategy=\"epoch\",\n",
    "                # Run\n",
    "                do_train=True,\n",
    "                do_eval=True,\n",
    "                # Training\n",
    "                num_train_epochs=4,            \n",
    "                max_steps=-1,\n",
    "                learning_rate=5e-5, # config\n",
    "                per_device_train_batch_size=32, # config\n",
    "                per_device_eval_batch_size=32,  # config\n",
    "                ## Learning rate scheduling\n",
    "    #             lr_scheduler_type = 'cosine',\n",
    "                warmup_steps=0,\n",
    "    #             warmup_ratio = 0.2,\n",
    "                ## Regularization\n",
    "                weight_decay=0.01, # config\n",
    "                # Logging\n",
    "                logging_dir='./logs',\n",
    "                report_to =\"none\",\n",
    "                # Evaluation\n",
    "                metric_for_best_model = 'auprc',\n",
    "                evaluation_strategy='epoch',\n",
    "                eval_steps = 500,           \n",
    "                # ETC    \n",
    "                load_best_model_at_end = True,\n",
    "                seed = 42,        \n",
    "                skip_memory_metrics=True,\n",
    "                # GPU\n",
    "                fp16 = True,\n",
    "                no_cuda=gpus_per_trial <= 0,\n",
    "                )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=get_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['dev'],\n",
    "        compute_metrics=build_compute_metrics_fn())\n",
    "\n",
    "    tune_config = {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n",
    "        \"max_steps\": 1 if smoke_test else -1,  # Used for smoke test.\n",
    "    }\n",
    "\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"eval_acc\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "            \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
    "            \"per_device_train_batch_size\": [16, 32, 64],\n",
    "        })\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns={\n",
    "            \"weight_decay\": \"w_decay\",\n",
    "            \"learning_rate\": \"lr\",\n",
    "            \"per_device_train_batch_size\": \"train_bs/gpu\",\n",
    "            \"num_train_epochs\": \"num_epochs\"\n",
    "        },\n",
    "        metric_columns=[\n",
    "            \"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"\n",
    "        ])\n",
    "\n",
    "    trainer.hyperparameter_search(\n",
    "        hp_space=lambda _: tune_config,\n",
    "        backend=\"ray\",\n",
    "        n_trials=num_samples,\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        scheduler=scheduler,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"training_iteration\",\n",
    "        stop={\"training_iteration\": 1} if smoke_test else None,\n",
    "        progress_reporter=reporter,\n",
    "        local_dir=\"~/ray_results/\",\n",
    "        name=\"tune_transformer_pbt\",\n",
    "        log_to_file=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    parser.add_argument(\n",
    "        \"--ray-address\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Address to use for Ray. \"\n",
    "        \"Use \\\"auto\\\" for cluster. \"\n",
    "        \"Defaults to None for local.\")\n",
    "    parser.add_argument(\n",
    "        \"--server-address\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"The address of server to connect to if using \"\n",
    "        \"Ray Client.\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.smoke_test:\n",
    "        ray.init()\n",
    "    elif args.server_address:\n",
    "        ray.init(f\"ray://{args.server_address}\")\n",
    "    else:\n",
    "        ray.init(args.ray_address)\n",
    "\n",
    "    if args.smoke_test:\n",
    "        tune_transformer(num_samples=1, gpus_per_trial=0, smoke_test=True)\n",
    "    else:\n",
    "        # You can change the number of GPUs here:\n",
    "        tune_transformer(num_samples=8, gpus_per_trial=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
